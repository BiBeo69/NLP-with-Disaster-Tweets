{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":29850,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T12:36:32.864521Z","iopub.execute_input":"2025-08-19T12:36:32.864691Z","iopub.status.idle":"2025-08-19T12:36:44.095502Z","shell.execute_reply.started":"2025-08-19T12:36:32.864657Z","shell.execute_reply":"2025-08-19T12:36:44.094517Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (2.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers) (1.18.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.85)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2020.1.8)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.38)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers) (4.40.2)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.10.50)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.13.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.7)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\nRequirement already satisfied: botocore<1.14.0,>=1.13.50 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (1.13.50)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.2.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.50->boto3->transformers) (2.8.1)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.50->boto3->transformers) (0.15.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (1.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch) (1.18.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Bidirectional, GRU, Dropout, LayerNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nimport transformers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# CUSTOM ATTENTION LAYER FOR SEQUENCE MODELING\nclass AttentionLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(name='attention_weight', \n                                shape=(input_shape[-1], 1),\n                                initializer='random_normal',\n                                trainable=True)\n        self.b = self.add_weight(name='attention_bias',\n                                shape=(input_shape[1], 1),\n                                initializer='zeros',\n                                trainable=True)\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, x):\n        e = K.tanh(K.dot(x, self.W) + self.b)\n        a = K.softmax(e, axis=1)\n        output = x * a\n        return K.sum(output, axis=1)\n\n# TEXT ENCODING FUNCTION FOR BERT TOKENIZATION\ndef bert_encode(texts, tokenizer, max_len=160):\n    all_tokens = []\n    all_masks = []\n    \n    for text in texts:\n        text = str(text)\n        tokens = tokenizer.tokenize(text)[:max_len-2]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        \n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        attention_mask = [1] * len(input_ids)\n        \n        pad_len = max_len - len(input_ids)\n        input_ids += [0] * pad_len\n        attention_mask += [0] * pad_len\n        \n        all_tokens.append(input_ids[:max_len])\n        all_masks.append(attention_mask[:max_len])\n    \n    return np.array(all_tokens), np.array(all_masks)\n\n# BUILD ADVANCED NEURAL NETWORK MODEL\ndef build_advanced_model(transformer, max_len=160):\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n    sequence_output = transformer(input_ids)[0]\n    \n    gru_output = Bidirectional(GRU(64, return_sequences=True, dropout=0.2, \n                                 recurrent_dropout=0.2))(sequence_output)\n    attention_output = AttentionLayer()(gru_output)\n    normalized = LayerNormalization()(attention_output)\n    \n    x = Dense(128, activation='relu')(normalized)\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    \n    out = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=input_ids, outputs=out)\n    optimizer = Adam(learning_rate=2e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n    \n    model.compile(optimizer=optimizer, \n                 loss='binary_crossentropy', \n                 metrics=['accuracy', \n                         tf.keras.metrics.Precision(name='precision'),\n                         tf.keras.metrics.Recall(name='recall')])\n    return model\n\n# LOAD AND PREPARE DATA\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\n# INITIALIZE TOKENIZER AND TRANSFORMER\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntransformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n\n# ENCODE TEXT DATA\ntrain_input, train_masks = bert_encode(train.text.values, tokenizer)\ntest_input, test_masks = bert_encode(test.text.values, tokenizer)\ntrain_labels = train.target.values\n\n# HANDLE CLASS IMBALANCE\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\nclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\nprint(f\"CLASS WEIGHTS: {class_weight_dict}\")\n\n# SETUP CROSS-VALIDATION\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\ntest_preds = np.zeros(len(test))\noof_preds = np.zeros(len(train))\noof_labels = np.zeros(len(train))\n\n# DEFINE TRAINING CALLBACKS\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-7, verbose=1)\n]\n\n# K-FOLD MODEL TRAINING\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_input, train_labels)):\n    print(f\"\\n{'='*50}\")\n    print(f\"FOLD {fold + 1}/{n_splits}\")\n    print(f\"{'='*50}\")\n    \n    tf.keras.backend.clear_session()\n    model = build_advanced_model(transformer_layer)\n    \n    X_train, X_val = train_input[train_idx], train_input[val_idx]\n    y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n    \n    history = model.fit(\n        X_train, \n        y_train,\n        validation_data=(X_val, y_val),\n        epochs=4,\n        batch_size=16,\n        callbacks=callbacks,\n        class_weight=class_weight_dict,\n        verbose=1\n    )\n    \n    val_pred = model.predict(X_val, verbose=0).flatten()\n    test_pred = model.predict(test_input, verbose=0).flatten()\n    \n    oof_preds[val_idx] = val_pred\n    oof_labels[val_idx] = y_val\n    test_preds += test_pred / n_splits\n    \n    val_pred_binary = (val_pred > 0.5).astype(int)\n    fold_f1 = f1_score(y_val, val_pred_binary)\n    fold_accuracy = np.mean(y_val == val_pred_binary)\n    \n    print(f\"FOLD {fold + 1} RESULTS:\")\n    print(f\"F1 SCORE: {fold_f1:.4f}\")\n    print(f\"ACCURACY: {fold_accuracy:.4f}\")\n    print(f\"VALIDATION LOSS: {history.history['val_loss'][-1]:.4f}\")\n\n# FINAL MODEL EVALUATION\nfinal_preds_binary = (oof_preds > 0.5).astype(int)\nfinal_f1 = f1_score(train_labels, final_preds_binary)\nfinal_accuracy = np.mean(train_labels == final_preds_binary)\n\nprint(f\"\\n{'='*60}\")\nprint(\"FINAL CROSS-VALIDATION RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"OVERALL OOF F1 SCORE: {final_f1:.4f}\")\nprint(f\"OVERALL OOF ACCURACY: {final_accuracy:.4f}\")\nprint(\"\\nCLASSIFICATION REPORT:\")\nprint(classification_report(train_labels, final_preds_binary))\n\n# CREATE SUBMISSION FILE\noptimal_threshold = 0.5\nsubmission['target'] = (test_preds > optimal_threshold).astype(int)\nsubmission.to_csv('submission.csv', index=False)\n\nprint(f\"\\nSUBMISSION CREATED WITH THRESHOLD {optimal_threshold}\")\nprint(f\"POSITIVE PREDICTIONS: {submission['target'].sum()}/{len(submission)}\")\nprint(\"FILE SAVED AS 'submission.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T12:36:56.061200Z","iopub.execute_input":"2025-08-19T12:36:56.061490Z","iopub.status.idle":"2025-08-19T13:45:09.421516Z","shell.execute_reply.started":"2025-08-19T12:36:56.061449Z","shell.execute_reply":"2025-08-19T13:45:09.420697Z"}},"outputs":[{"name":"stdout","text":"Class weights: {0: 0.8766697374481806, 1: 1.1637114032405993}\n\n==================================================\nFold 1/5\n==================================================\nTrain on 6089 samples, validate on 1524 samples\nEpoch 1/4\n6089/6089 [==============================] - 238s 39ms/sample - loss: 0.6940 - accuracy: 0.5405 - precision: 0.4731 - recall: 0.6124 - val_loss: 0.4997 - val_accuracy: 0.7756 - val_precision: 0.6852 - val_recall: 0.8840\nEpoch 2/4\n6089/6089 [==============================] - 223s 37ms/sample - loss: 0.5093 - accuracy: 0.7720 - precision: 0.7352 - recall: 0.7336 - val_loss: 0.4165 - val_accuracy: 0.8281 - val_precision: 0.7835 - val_recall: 0.8290\nEpoch 3/4\n6089/6089 [==============================] - 227s 37ms/sample - loss: 0.4105 - accuracy: 0.8326 - precision: 0.8089 - recall: 0.7993 - val_loss: 0.4054 - val_accuracy: 0.8333 - val_precision: 0.8652 - val_recall: 0.7252\nEpoch 4/4\n6080/6089 [============================>.] - ETA: 0s - loss: 0.3565 - accuracy: 0.8715 - precision: 0.8667 - recall: 0.8285\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n6089/6089 [==============================] - 227s 37ms/sample - loss: 0.3564 - accuracy: 0.8714 - precision: 0.8665 - recall: 0.8284 - val_loss: 0.4188 - val_accuracy: 0.8327 - val_precision: 0.8968 - val_recall: 0.6901\nFold 1 Results:\nF1 Score: 0.7800\nAccuracy: 0.8327\nValidation Loss: 0.4188\n\n==================================================\nFold 2/5\n==================================================\nTrain on 6090 samples, validate on 1523 samples\nEpoch 1/4\n6090/6090 [==============================] - 241s 40ms/sample - loss: 0.3717 - accuracy: 0.8575 - precision: 0.8360 - recall: 0.8315 - val_loss: 0.2874 - val_accuracy: 0.8831 - val_precision: 0.8343 - val_recall: 0.9083\nEpoch 2/4\n6090/6090 [==============================] - 228s 37ms/sample - loss: 0.3075 - accuracy: 0.8872 - precision: 0.8749 - recall: 0.8605 - val_loss: 0.2767 - val_accuracy: 0.9015 - val_precision: 0.8950 - val_recall: 0.8731\nEpoch 3/4\n6080/6090 [============================>.] - ETA: 0s - loss: 0.2558 - accuracy: 0.9179 - precision: 0.9155 - recall: 0.8913\nEpoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n6090/6090 [==============================] - 227s 37ms/sample - loss: 0.2555 - accuracy: 0.9181 - precision: 0.9156 - recall: 0.8915 - val_loss: 0.2945 - val_accuracy: 0.8936 - val_precision: 0.8761 - val_recall: 0.8761\nEpoch 4/4\n6080/6090 [============================>.] - ETA: 0s - loss: 0.2016 - accuracy: 0.9396 - precision: 0.9407 - recall: 0.9173Restoring model weights from the end of the best epoch.\n\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n6090/6090 [==============================] - 227s 37ms/sample - loss: 0.2014 - accuracy: 0.9397 - precision: 0.9408 - recall: 0.9175 - val_loss: 0.3133 - val_accuracy: 0.9002 - val_precision: 0.9010 - val_recall: 0.8624\nEpoch 00004: early stopping\nFold 2 Results:\nF1 Score: 0.8839\nAccuracy: 0.9015\nValidation Loss: 0.3133\n\n==================================================\nFold 3/5\n==================================================\nTrain on 6091 samples, validate on 1522 samples\nEpoch 1/4\n6091/6091 [==============================] - 241s 40ms/sample - loss: 0.3375 - accuracy: 0.8811 - precision: 0.8862 - recall: 0.8300 - val_loss: 0.1963 - val_accuracy: 0.9323 - val_precision: 0.9584 - val_recall: 0.8807\nEpoch 2/4\n6080/6091 [============================>.] - ETA: 0s - loss: 0.2715 - accuracy: 0.9141 - precision: 0.9259 - recall: 0.8699\nEpoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n6091/6091 [==============================] - 227s 37ms/sample - loss: 0.2712 - accuracy: 0.9143 - precision: 0.9260 - recall: 0.8701 - val_loss: 0.2058 - val_accuracy: 0.9310 - val_precision: 0.9378 - val_recall: 0.8991\nEpoch 3/4\n6080/6091 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.9378 - precision: 0.9501 - recall: 0.9028Restoring model weights from the end of the best epoch.\n\nEpoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n6091/6091 [==============================] - 227s 37ms/sample - loss: 0.2137 - accuracy: 0.9378 - precision: 0.9501 - recall: 0.9026 - val_loss: 0.2062 - val_accuracy: 0.9297 - val_precision: 0.9390 - val_recall: 0.8945\nEpoch 00003: early stopping\nFold 3 Results:\nF1 Score: 0.9179\nAccuracy: 0.9323\nValidation Loss: 0.2062\n\n==================================================\nFold 4/5\n==================================================\nTrain on 6091 samples, validate on 1522 samples\nEpoch 1/4\n6091/6091 [==============================] - 241s 39ms/sample - loss: 0.2815 - accuracy: 0.9128 - precision: 0.9136 - recall: 0.8804 - val_loss: 0.1959 - val_accuracy: 0.9409 - val_precision: 0.9393 - val_recall: 0.9220\nEpoch 2/4\n6080/6091 [============================>.] - ETA: 0s - loss: 0.2340 - accuracy: 0.9266 - precision: 0.9274 - recall: 0.8997\nEpoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n6091/6091 [==============================] - 227s 37ms/sample - loss: 0.2340 - accuracy: 0.9266 - precision: 0.9272 - recall: 0.8999 - val_loss: 0.2100 - val_accuracy: 0.9336 - val_precision: 0.9247 - val_recall: 0.9205\nEpoch 3/4\n6080/6091 [============================>.] - ETA: 0s - loss: 0.1798 - accuracy: 0.9474 - precision: 0.9491 - recall: 0.9273Restoring model weights from the end of the best epoch.\n\nEpoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n6091/6091 [==============================] - 227s 37ms/sample - loss: 0.1796 - accuracy: 0.9475 - precision: 0.9492 - recall: 0.9274 - val_loss: 0.2304 - val_accuracy: 0.9376 - val_precision: 0.9415 - val_recall: 0.9113\nEpoch 00003: early stopping\nFold 4 Results:\nF1 Score: 0.9306\nAccuracy: 0.9409\nValidation Loss: 0.2304\n\n==================================================\nFold 5/5\n==================================================\nTrain on 6091 samples, validate on 1522 samples\nEpoch 1/4\n6091/6091 [==============================] - 242s 40ms/sample - loss: 0.2734 - accuracy: 0.9151 - precision: 0.9118 - recall: 0.8884 - val_loss: 0.1436 - val_accuracy: 0.9560 - val_precision: 0.9550 - val_recall: 0.9419\nEpoch 2/4\n6080/6091 [============================>.] - ETA: 0s - loss: 0.2269 - accuracy: 0.9337 - precision: 0.9351 - recall: 0.9090\nEpoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n6091/6091 [==============================] - 228s 37ms/sample - loss: 0.2267 - accuracy: 0.9337 - precision: 0.9348 - recall: 0.9091 - val_loss: 0.1506 - val_accuracy: 0.9488 - val_precision: 0.9377 - val_recall: 0.9434\nEpoch 3/4\n6080/6091 [============================>.] - ETA: 0s - loss: 0.1801 - accuracy: 0.9495 - precision: 0.9449 - recall: 0.9373Restoring model weights from the end of the best epoch.\n\nEpoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n6091/6091 [==============================] - 228s 37ms/sample - loss: 0.1798 - accuracy: 0.9496 - precision: 0.9449 - recall: 0.9373 - val_loss: 0.1538 - val_accuracy: 0.9527 - val_precision: 0.9450 - val_recall: 0.9450\nEpoch 00003: early stopping\nFold 5 Results:\nF1 Score: 0.9484\nAccuracy: 0.9560\nValidation Loss: 0.1538\n\n============================================================\nFINAL CROSS-VALIDATION RESULTS:\n============================================================\nOverall OOF F1 Score: 0.8945\nOverall OOF Accuracy: 0.9126\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.95      0.93      4342\n           1       0.93      0.86      0.89      3271\n\n    accuracy                           0.91      7613\n   macro avg       0.92      0.91      0.91      7613\nweighted avg       0.91      0.91      0.91      7613\n\n\nSubmission created with threshold 0.5\nPositive predictions: 1257/3263\nFile saved as 'submission.csv'\n","output_type":"stream"}],"execution_count":2}]}